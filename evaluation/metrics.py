"""
Evaluation metrics for the Fortune 500 RAG Chatbot.

This module provides functions for calculating various evaluation metrics
including accuracy, completeness, and latency.
"""

import re
import time
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

def clean_text_for_comparison(text: str) -> str:
    """
    Clean and normalize text for comparison purposes.
    
    Args:
        text: Text to clean
        
    Returns:
        str: Cleaned text
    """
    if not text:
        return ""
        
    # Convert to lowercase
    text = text.lower()
    
    # Replace multiple dollar signs with a single one
    text = re.sub(r'\$+\.?\$+', '$', text)  # Match patterns like "$$.$"
    text = re.sub(r'\${2,}', '$', text)
    
    # Normalize financial notation
    text = re.sub(r'(\d+)\s*billion', r'\1b', text)
    text = re.sub(r'(\d+)\s*million', r'\1m', text)
    
    # Normalize spaces
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def calculate_latency(func, *args, **kwargs) -> Dict[str, float]:
    """
    Measure the execution time of a function.
    
    Args:
        func: The function to measure
        *args, **kwargs: Arguments to pass to the function
        
    Returns:
        Dict: A dictionary with latency information
    """
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    
    return {
        "latency_seconds": end_time - start_time,
        "result": result
    }

def calculate_fact_coverage(response: str, ground_truth_facts: List[str]) -> Dict[str, Any]:
    """
    Calculate what percentage of ground truth facts are covered in the response.
    
    Args:
        response: The generated response text
        ground_truth_facts: List of facts that should be present
        
    Returns:
        Dict: Coverage metrics
    """
    total_facts = len(ground_truth_facts)
    covered_facts = 0
    covered_fact_details = []
    
    # Clean the response for better matching
    clean_response = clean_text_for_comparison(response)
    
    for fact in ground_truth_facts:
        # Clean the fact for better matching
        clean_fact = clean_text_for_comparison(fact)
        
        # Check if the key components of the fact are in the response
        fact_words = set(re.sub(r'[^\w\s]', '', clean_fact).split())
        response_words = set(re.sub(r'[^\w\s]', '', clean_response).split())
        
        # Calculate word overlap
        overlap = len(fact_words.intersection(response_words)) / len(fact_words) if fact_words else 0
        
        # Consider a fact covered if there's significant word overlap
        is_covered = overlap >= 0.7
        
        if is_covered:
            covered_facts += 1
            covered_fact_details.append({
                "fact": fact,
                "covered": True,
                "overlap_score": overlap
            })
        else:
            covered_fact_details.append({
                "fact": fact,
                "covered": False,
                "overlap_score": overlap
            })
    
    return {
        "coverage_percentage": (covered_facts / total_facts) * 100 if total_facts > 0 else 0,
        "covered_facts": covered_facts,
        "total_facts": total_facts,
        "fact_details": covered_fact_details
    }

def evaluate_with_llm(response: str, question: str, expected_answer: str, 
                      llm_model: str = "gpt-4o") -> Dict[str, Any]:
    """
    Use an LLM to evaluate the quality of a response.
    
    Args:
        response: The generated response to evaluate
        question: The original question
        expected_answer: The expected answer for comparison
        llm_model: The LLM model to use for evaluation
        
    Returns:
        Dict: Evaluation metrics from the LLM
    """
    # Clean response for display
    cleaned_response = re.sub(r'\$+\.?\$+', '$', response)  # Match patterns like "$$.$"
    cleaned_response = re.sub(r'\${2,}', '$', cleaned_response)
    
    # Initialize the LLM
    llm = ChatOpenAI(temperature=0, model=llm_model)
    
    # Create evaluation prompt
    eval_prompt = PromptTemplate(
        input_variables=["question", "expected_answer", "actual_answer"],
        template="""
        You are an expert evaluator for question answering systems. You will be given a question, 
        the expected answer, and the actual answer generated by an AI system. 
        
        Evaluate the actual answer based on the following criteria:
        1. Factual Accuracy: Are all factual claims in the actual answer correct according to the expected answer?
        2. Completeness: Does the actual answer include all key information from the expected answer?
        3. Relevance: How directly does the actual answer address the question?
        
        Question: {question}
        Expected Answer: {expected_answer}
        Actual Answer: {actual_answer}
        
        Provide your evaluation in the following JSON format:
        {{
            "factual_accuracy_score": <score from 0 to 10>,
            "completeness_score": <score from 0 to 10>,
            "relevance_score": <score from 0 to 10>,
            "overall_score": <average of all scores>,
            "factual_errors": [<list of any factual errors>],
            "missing_information": [<list of important missing information>],
            "explanation": "<brief explanation of your evaluation>"
        }}
        
        Return only the JSON object without any additional text.
        """
    )
    
    # Format the prompt
    formatted_prompt = eval_prompt.format(
        question=question,
        expected_answer=expected_answer,
        actual_answer=cleaned_response
    )
    
    # Get LLM evaluation
    try:
        eval_result = llm.invoke(formatted_prompt)
        eval_text = eval_result.content
        
        # Extract JSON section if there's any additional text
        json_pattern = r'({[\s\S]*})'
        match = re.search(json_pattern, eval_text)
        if match:
            eval_text = match.group(1)
            
        # Convert to dictionary (in a real implementation, handle potential 
        # parsing errors and make this more robust)
        import json
        eval_metrics = json.loads(eval_text)
        
        return eval_metrics
    except Exception as e:
        print(f"Error during LLM evaluation: {e}")
        # Return default metrics in case of error
        return {
            "factual_accuracy_score": 0,
            "completeness_score": 0,
            "relevance_score": 0,
            "overall_score": 0,
            "error": str(e)
        }

def evaluate_with_llm_with_retry(response: str, question: str, expected_answer: str, 
                                llm_model: str = "gpt-4o", max_retries: int = 3) -> Dict[str, Any]:
    """
    Use an LLM to evaluate the quality of a response with retry logic.
    
    Args:
        response: The generated response to evaluate
        question: The original question
        expected_answer: The expected answer for comparison
        llm_model: The LLM model to use for evaluation
        max_retries: Maximum number of retry attempts
        
    Returns:
        Dict: Evaluation metrics from the LLM
    """
    for attempt in range(max_retries):
        try:
            return evaluate_with_llm(response, question, expected_answer, llm_model)
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"Retry {attempt+1}/{max_retries} after error: {e}")
                time.sleep(2)  # Wait before retrying
                continue
            else:
                # Return a default evaluation on failure
                return {
                    "factual_accuracy_score": 0,
                    "completeness_score": 0,
                    "relevance_score": 0,
                    "overall_score": 0,
                    "error": str(e)
                }

def calculate_aggregate_metrics(evaluation_results: List[Dict]) -> Dict[str, Any]:
    """
    Calculate aggregate metrics across all evaluation results.
    
    Args:
        evaluation_results: List of individual evaluation results
        
    Returns:
        Dict: Aggregate metrics
    """
    if not evaluation_results:
        return {"error": "No evaluation results provided"}
    
    # Extract metrics
    latencies = [result.get("latency", {}).get("total_seconds", 0) 
                for result in evaluation_results]
    avg_latency = sum(latencies) / len(latencies) if latencies else 0
    
    avg_coverage = np.mean([result.get("fact_coverage", {}).get("coverage_percentage", 0) 
                           for result in evaluation_results])
    
    # Get LLM evaluation metrics if available
    llm_metrics = {}
    llm_eval_count = 0
    
    for result in evaluation_results:
        if "llm_evaluation" in result:
            llm_eval = result["llm_evaluation"]
            llm_eval_count += 1
            
            for key, value in llm_eval.items():
                if (isinstance(value, (int, float)) and 
                    "error" not in key.lower()):
                    llm_metrics[key] = llm_metrics.get(key, 0) + value
    
    # Calculate averages for LLM metrics
    if llm_eval_count > 0:
        for key in llm_metrics:
            llm_metrics[key] = llm_metrics[key] / llm_eval_count
    
    return {
        "total_questions": len(evaluation_results),
        "average_latency_seconds": avg_latency,
        "average_fact_coverage_percentage": avg_coverage,
        "llm_evaluation_averages": llm_metrics,
        "individual_results_count": len(evaluation_results)
    }